{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from functools import reduce\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('data/estadao_noticias_eleicao.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join do conteúdo\n",
    "Juntando os títulos das notícias com seus respectivos conteúdos,\n",
    "para posteriomente facilitar a tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "    texto = normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    return pattern.sub(' ', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "materias = dados.titulo + \" \" + dados.subTitulo +  \" \" + dados.conteudo\n",
    "materias = materias.apply(lambda texto: \"\" if isinstance(texto, float) else limpar_texto(texto))\n",
    "ids = dados.idNoticia\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizando conteúdo\n",
    "Criando tokens com cada palavra do texto para que posteriormente possam ser indexadas e associadas aos respectivos ids das notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = materias.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexando tokens\n",
    "Criando indices invertidos com os tokens para poder aplicar os métodos de busca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    id_noticia = ids[i]\n",
    "    palavras = tokens[i]\n",
    "    for palavra in palavras:\n",
    "        palavra = palavra.lower()\n",
    "        if palavra not in index:\n",
    "            index[palavra] = []\n",
    "        \n",
    "        id_rec = reduce(lambda found, id_not: id_not if id_not[1] == id_noticia else found, index.get(palavra, []), None)\n",
    "        \n",
    "        if not id_rec:\n",
    "            docs = index[palavra]\n",
    "            docs.append((1, id_noticia))\n",
    "        else:\n",
    "            docs = index[palavra]\n",
    "            index_id = docs.index(id_rec)\n",
    "            docs[index_id] = (id_rec[0] + 1, id_rec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_tf_vetor(frase):\n",
    "    termos = frase.split(\" \")\n",
    "    doc_tf = {}\n",
    "    \n",
    "    for i in range(len(termos)):\n",
    "        termo = termos[i]\n",
    "        docs = index[termo]\n",
    "        for doc in docs:\n",
    "            doc_id = doc[1]\n",
    "            tf = doc[0]\n",
    "            \n",
    "            if doc_id not in doc_tf:\n",
    "                doc_tf[doc_id] = np.array([0 if j != i else tf for j in range(len(termos))])\n",
    "            else:\n",
    "                doc_vector = doc_tf[doc_id]\n",
    "                doc_tf[doc_id] = np.array([doc_vector[j] if j != i else tf for j in range(len(termos))])\n",
    "        \n",
    "    return doc_tf\n",
    "\n",
    "def gera_idf_vetor(frase):\n",
    "    termos = frase.split(\" \")\n",
    "    idf_vector = np.array([math.log((len(materias)+1)/len(index[termo])) for termo in termos])\n",
    "    return idf_vector\n",
    "\n",
    "def gera_query_vetor(frase):\n",
    "    termos = frase.split(\" \")\n",
    "    vetor = np.array([1 if index.get(termo) else 0 for termo in termos])\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_por_tf(frase):\n",
    "    docs_tf = gera_tf_vetor(frase)\n",
    "    query = gera_query_vetor(frase)\n",
    "    \n",
    "    return sorted(list(docs_tf.items()), key=lambda doc: np.dot(doc[1], query), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_por_tf_idf(frase):\n",
    "    docs_tf = gera_tf_vetor(frase)\n",
    "    idf = gera_idf_vetor(frase)\n",
    "    \n",
    "    return sorted(list(docs_tf.items()), key=lambda doc: np.dot(doc[1], idf), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2744, array([18, 25])), (7672, array([12, 14])), (2388, array([13, 11])), (2178, array([11, 11])), (1235, array([ 1, 17]))]\n",
      "[(2744, array([18, 25])), (7672, array([12, 14])), (2388, array([13, 11])), (1235, array([ 1, 17])), (2178, array([11, 11]))]\n"
     ]
    }
   ],
   "source": [
    "# print(gera_idf_vetor('lula preso'))\n",
    "# print(gera_tf_vetor('lula preso'))\n",
    "# print(len(materias))\n",
    "# print(len(index['preso']))\n",
    "# print(len(materias)/len(index['preso']))\n",
    "print(buscar_por_tf('segundo turno'))\n",
    "print(buscar_por_tf_idf('segundo turno'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
