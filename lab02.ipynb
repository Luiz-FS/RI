{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from nltk import bigrams    \n",
    "from unicodedata import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema: Expansão de Consultas\n",
    "### Autor: Luiz Fernando da Silva\n",
    "Neste laboratório analizarei um conjunto de notícias do estadão armazenados em um arquivo csv utilizando técnicas de expansão de consultas para, por fim, responder as seguinstes questões:\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('data/estadao_noticias_eleicao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "    texto = normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    return pattern.sub(' ', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [limpar_texto(stopword) for stopword in stopwords.words('portuguese')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteudo = dados.titulo + \" \" + dados.subTitulo + \" \" + dados.conteudo\n",
    "conteudo = conteudo.fillna(\"\")\n",
    "conteudo = conteudo.apply(limpar_texto)\n",
    "ids = dados.idNoticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = conteudo.apply(nltk.word_tokenize)\n",
    "term_frequence = tokens.apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    id_noticia = ids[i]\n",
    "    palavras = tokens[i]\n",
    "    for palavra in palavras:\n",
    "        palavra = palavra.lower()\n",
    "        if palavra not in index:\n",
    "            index[palavra] = {}\n",
    "        \n",
    "        id_rec = index[palavra].get(id_noticia)\n",
    "        \n",
    "        if not id_rec:\n",
    "            docs = index[palavra]\n",
    "            docs[id_noticia] = term_frequence[i][palavra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_docs_peso(termos):\n",
    "    docs_peso = {}\n",
    "    \n",
    "    for i in range(len(termos)):\n",
    "        termo = termos[i]\n",
    "        docs = index[termo]\n",
    "        for doc_id in docs:\n",
    "            tf = docs[doc_id]\n",
    "            \n",
    "            if doc_id not in docs_peso:\n",
    "                docs_peso[doc_id] = np.array([0 if j != i else tf for j in range(len(termos))])\n",
    "            else:\n",
    "                doc_vector = docs_peso[doc_id]\n",
    "                doc_vector[i] = tf\n",
    "    return docs_peso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_query_vetor(termos):\n",
    "    vetor = np.array([1 if index.get(termo) else 0 for termo in termos])\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca(termos, gerador_query, gerador_doc_vetor):\n",
    "    docs_peso = gerador_doc_vetor(termos)\n",
    "    query = gerador_query(termos)\n",
    "    \n",
    "    doc_rank = sorted(list(docs_peso.items()), key=lambda doc: np.dot(doc[1], query), reverse=True) \n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_por_tf(termos):\n",
    "    return busca(termos, gera_query_vetor, gera_docs_peso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função para gerar matrix esparsa de coocorrência\n",
    "Este código pode ser encontrado em [https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb](https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lists = conteudo.apply(lambda text: text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for tokens_list in tokens_lists for token in tokens_list if token not in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consult Bigram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_ocurrence(word):\n",
    "    list_of_occurency = consultable_matrix[vocab[word]].getrow(0).toarray()[0]\n",
    "    indexs, frequency = zip(*sorted(enumerate(list_of_occurency), key=lambda x: x[1], reverse=True))\n",
    "    return indexs[:3], frequency[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_expasao(termo):\n",
    "    ocurrecy = get_co_ocurrence(termo)\n",
    "    expansao = [word for key in ocurrecy[0] for word in vocab.keys() if vocab[word] == key]\n",
    "    expansao.append(termo)\n",
    "    exp = buscar_por_tf(expansao)\n",
    "    return (expansao, exp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando Busca Expansiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rousseff', 'disse', 'aecio', 'dilma']\n",
      "3853\n",
      "2669\n",
      "['paulo', 'graca', 'disse', 'petrobras']\n",
      "3886\n",
      "963\n",
      "['neves', 'disse', 'afirmou', 'aecio']\n",
      "3658\n",
      "1578\n"
     ]
    }
   ],
   "source": [
    "# Usado o termo Dilma\n",
    "termo = 'dilma'\n",
    "expansao, doc_exp = busca_expasao(termo)\n",
    "busca_original = buscar_por_tf([termo])\n",
    "print(expansao)\n",
    "print(len(doc_exp))\n",
    "print(len(busca_original))\n",
    "\n",
    "# Usado o termo petrobras\n",
    "termo = 'petrobras'\n",
    "expansao, doc_exp = busca_expasao(termo)\n",
    "busca_original = set(buscar_por_tf([termo]))\n",
    "print(expansao)\n",
    "print(len(doc_exp))\n",
    "print(len(busca_original))\n",
    "\n",
    "# Usado o termo aecio\n",
    "termo = 'aecio'\n",
    "expansao, doc_exp = busca_expasao(termo)\n",
    "busca_original = set(buscar_por_tf([termo]))\n",
    "print(expansao)\n",
    "print(len(doc_exp))\n",
    "print(len(busca_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise\n",
    "\n",
    "**Tabela dos resultados das buscas originais e expandidas.**\n",
    "\n",
    "| Termo     | Termos expanão         | Busca Original | Busca Expandida|\n",
    "|---------- |:----------------------:|:--------------:| --------------:|\n",
    "| Dilma     | rousseff, disse, aecio | 2669           | 3853           |\n",
    "| Petrobrás | paulo, graca, disse    | 963            | 3886           |\n",
    "| Aécio     | neves, disse, afirmou  | 1578           | 3658           |\n",
    "\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "> Observando a tablea de resultados acima temos que os termos retornados para as buscas de Dilma, Petrobrás e Aécio são:\n",
    "> * Dilma\n",
    ">> * rousseff \n",
    ">> * disse \n",
    ">> * aecio\n",
    "> * Petrobrás\n",
    ">> * paulo \n",
    ">> * graca\n",
    ">> * disse\n",
    "> * Aécio\n",
    ">> * neves\n",
    ">> * disse\n",
    ">> * afirmou\n",
    "\n",
    "\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "> <p style=\"text-align: justify\">Os termos retornados estão relacionados com as consultas pois, são eles quem mais aparecem em conjunto com a busca desejada e podem se gerar uma maior aproximação com que o usuário deseja pesquisar. Podemo ver com mais clareza na tabela de resultados acima, onde ao pesquisar por dilma um dos resultados dos termos de expansão é Rousseff, que é seu sobrenome. Outra análise que pode ser feita é calcular quantos dos documentos retornados pela busca original também são retornados pela busca expandida. Neste caso, mesmo não adicionado os termos da consulta original e deixando apenas os termos que mais estão correlacionados para fazer a busca, a quantidade de documentos que estão na busca original e que não estão na busca expandida é mínima, o que mostra que esses termos estão bastante relacionados aos termos da consulta original.</p>\n",
    "\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "> <p style=\"text-align: justify\">A busca expandida retorna resultados melhores para a busca, pois como ela usa os termos que estão mais relacionados com os termos originais, fica mais expressiva e retornará também resultados de outras possíveis palavras chave que o usuário poderia usar em futuras buscas. Entretanto, quanto mais termos a busca expandida tiver, maior vai ser a quantidade de documentos retornados por ela, e se, não houver um rankeamento adequado dos resultados, o usuário pode levar mais tempo procurando os documentos que realmente são relevantes para ele.</p>\n",
    "\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n",
    "> <p style=\"text-align: justify\">Expandindo a consulta o recall irá melhorar, pois ele é a razão entre o total de documentos relevantes na busca e e a quantidade total de documentos relevantes, ou seja, quanto maior for a quantidade de documentos retornados maior será a quantidade de documentos relevantes que estarão contidos nessa busca. Por outro lado, a precisão diminui já que ela é a razão entre a quantidade de documentos relevantes na busca e a quantidade de documentos totais presentes na busca. Formula do recall: $\\frac{tp}{tp+fn}$; formula do precision: $\\frac{tp}{tp+fp}$; onde, tp é o total de documentos relevantes na busca, fn é o total de documentos relevantes não presentes na busca e fp é o total de documentos não relevantes na busca.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
