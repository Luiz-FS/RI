{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from nltk import bigrams    \n",
    "from unicodedata import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema: Expansão de Consultas\n",
    "### Autor: Luiz Fernando da Silva\n",
    "Neste laboratório analizarei um conjunto de notícias do estadão armazenados em um arquivo csv utilizando técnicas de expansão de consultas para, por fim, responder as seguinstes questões:\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('data/estadao_noticias_eleicao.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função para limpar o texto\n",
    "Essa Função remove todos os caracteres especiais do texto bem como sua acentuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "    texto = normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    return pattern.sub(' ', texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de stopwords\n",
    "Criando lista de stopwords à ser removida da matrix de coocorrência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [limpar_texto(stopword) for stopword in stopwords.words('portuguese')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join do conteúdo\n",
    "Juntando os títulos das notícias com seus respectivos subtítulos e conteúdos, e também removendo das nóticias os caracteres especiais e a acentuação para posteriomente facilitar a tokenização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteudo = dados.titulo + \" \" + dados.subTitulo + \" \" + dados.conteudo\n",
    "conteudo = conteudo.fillna(\"\")\n",
    "conteudo = conteudo.apply(limpar_texto)\n",
    "ids = dados.idNoticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizando conteúdo\n",
    "Criando tokens com cada palavra do texto para que posteriormente possam ser indexadas e associadas aos respectivos ids das notícias, e contando a frequência de cada termo no texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = conteudo.apply(nltk.word_tokenize)\n",
    "term_frequence = tokens.apply(Counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexando tokens\n",
    "Criando indices invertidos com os tokens para poder aplicar os métodos de busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    id_noticia = ids[i]\n",
    "    palavras = tokens[i]\n",
    "    for palavra in palavras:\n",
    "        palavra = palavra.lower()\n",
    "        if palavra not in index:\n",
    "            index[palavra] = {}\n",
    "        \n",
    "        id_rec = index[palavra].get(id_noticia)\n",
    "        \n",
    "        if not id_rec:\n",
    "            docs = index[palavra]\n",
    "            docs[id_noticia] = term_frequence[i][palavra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método que gera um dicionário com vetores de pesos\n",
    "Este método gera uma dicionário cujas chaves são os ids documentos que contém os termos pesquisados e os valores são vetores que contem o peso de cada termo nos documentos, esse vetor é do tipo TF (Term Frequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_docs_peso(termos):\n",
    "    docs_peso = {}\n",
    "    \n",
    "    for i in range(len(termos)):\n",
    "        termo = termos[i]\n",
    "        docs = index[termo]\n",
    "        for doc_id in docs:\n",
    "            tf = docs[doc_id]\n",
    "            \n",
    "            if doc_id not in docs_peso:\n",
    "                docs_peso[doc_id] = np.array([0 if j != i else tf for j in range(len(termos))])\n",
    "            else:\n",
    "                doc_vector = docs_peso[doc_id]\n",
    "                doc_vector[i] = tf\n",
    "    return docs_peso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método que gera um vetor binário da consulta\n",
    "Este método verifica se cada termo existe no indice e gera uma vetor binpario onde cada indice do vetor assume o valor de 0 se não existir ou 1 se existir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_query_vetor(termos):\n",
    "    vetor = np.array([1 if index.get(termo) else 0 for termo in termos])\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca genérica\n",
    "Este método encapsula as partes em comum de todos os métodos de busca implementados para evitar repetição de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca(termos, gerador_query, gerador_doc_vetor):\n",
    "    docs_peso = gerador_doc_vetor(termos)\n",
    "    query = gerador_query(termos)\n",
    "    \n",
    "    doc_rank = sorted(list(docs_peso.items()), key=lambda doc: np.dot(doc[1], query), reverse=True) \n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca por term frequence (TF)\n",
    "Busca vetorial pelo método TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_por_tf(termos):\n",
    "    return busca(termos, gera_query_vetor, gera_docs_peso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função para gerar matrix esparsa de coocorrência\n",
    "Este código pode ser encontrado em: [https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb](https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando tokens para criar a matrix esparsa\n",
    "Neste trexo são criados sos tokens que serão utilizados no método co_ocurrence para criar matrix esparsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lists = conteudo.apply(lambda text: text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for tokens_list in tokens_lists for token in tokens_list if token not in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando matrix esparsa\n",
    "Criando matrix esparsa para ser usada nas buscas expansivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix de bigramas consultável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gera lista de coocorrência e lista de frequência\n",
    "Essa função gera uma lista dos indices das palavras que mais coocorrem com o termo pesquisado ordenada em ordem decrescente pegando as primeiras 3 indeces de termos mais frequêntes e também gera uma lista com a respectiva frequência desses termos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_ocurrence(word):\n",
    "    list_of_occurency = consultable_matrix[vocab[word]].getrow(0).toarray()[0]\n",
    "    indexs, frequency = zip(*sorted(enumerate(list_of_occurency), key=lambda x: x[1], reverse=True))\n",
    "    return indexs[:3], frequency[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodo que faz uma busca expansiva\n",
    "Usando o método get_co_ocurrence é gerada uma lista dos termos que mais coocorrem com o termo pesquisado, é feita uma busca utilizando os novos termos encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_expansao(termo):\n",
    "    ocurrecy = get_co_ocurrence(termo)\n",
    "    expansao = [word for key in ocurrecy[0] for word in vocab.keys() if vocab[word] == key]\n",
    "    expansao.append(termo)\n",
    "    exp = buscar_por_tf(expansao)\n",
    "    return (expansao, exp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando Busca Expansiva\n",
    "Aqui é gerada uma tabela contendo os resultados da busca expansiva e da busca original dos termos dilma, petrobras e aecio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**1. Tabela dos resultados das buscas originais e expandidas**\n",
       "\n",
       "| Termo         | Termos expanão | Busca Original | Busca Expandida   |\n",
       "|:-------------:|:--------------:|:--------------:|:-----------------:|\n",
       "| dilma  | rousseff, disse, aecio    | 2669    | 3853  |\n",
       "| petrobras | paulo, graca, disse   | 963   | 3886 |\n",
       "| aecio  | neves, disse, afirmou    | 1578    | 3658  |\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando o termo Dilma\n",
    "termo1 = 'dilma'\n",
    "expansao1, doc_exp1 = busca_expansao(termo1)\n",
    "busca_original1 = buscar_por_tf([termo1])\n",
    "\n",
    "# Usando o termo petrobras\n",
    "termo2 = 'petrobras'\n",
    "expansao2, doc_exp2 = busca_expansao(termo2)\n",
    "busca_original2 = set(buscar_por_tf([termo2]))\n",
    "\n",
    "# Usanndo o termo aecio\n",
    "termo3 = 'aecio'\n",
    "expansao3, doc_exp3 = busca_expansao(termo3)\n",
    "busca_original3 = set(buscar_por_tf([termo3]))\n",
    "\n",
    "Markdown(\n",
    "\"\"\"\n",
    "**1. Tabela dos resultados das buscas originais e expandidas**\n",
    "\n",
    "| Termo         | Termos expansão | Busca Original | Busca Expandida   |\n",
    "|:-------------:|:---------------:|:--------------:|:-----------------:|\n",
    "| {first_term}  | {first_exp}     | {first_ori}    | {first_srch_exp}  |\n",
    "| {second_term} | {second_exp}    | {second_ori}   | {second_srch_exp} |\n",
    "| {third_term}  | {third_exp}     | {third_ori}    | {third_srch_exp}  |\n",
    "\n",
    "\"\"\".format(\n",
    "    first_term=termo1,\n",
    "    second_term=termo2,\n",
    "    third_term=termo3,\n",
    "    first_exp=', '.join(expansao1[:3]),\n",
    "    second_exp=', '.join(expansao2[:3]),\n",
    "    third_exp=', '.join(expansao3[:3]),\n",
    "    first_ori=len(busca_original1),\n",
    "    second_ori=len(busca_original2),\n",
    "    third_ori=len(busca_original3),\n",
    "    first_srch_exp=len(doc_exp1),\n",
    "    second_srch_exp=len(doc_exp2),\n",
    "    third_srch_exp=len(doc_exp3),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos resultados\n",
    "\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "> Observando a tabela de resultados acima temos que os termos retornados para as buscas de Dilma, Petrobrás e Aécio são:\n",
    "> * Dilma\n",
    ">> * rousseff \n",
    ">> * disse \n",
    ">> * aecio\n",
    "> * Petrobrás\n",
    ">> * paulo \n",
    ">> * graca\n",
    ">> * disse\n",
    "> * Aécio\n",
    ">> * neves\n",
    ">> * disse\n",
    ">> * afirmou\n",
    "\n",
    "\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "> <p style=\"text-align: justify\">Os termos retornados estão relacionados com as consultas pois, são eles quem mais aparecem em conjunto com a busca desejada e podem se gerar uma maior aproximação com que o usuário deseja pesquisar. Podemo ver com mais clareza na tabela de resultados acima, onde ao pesquisar por dilma um dos resultados dos termos de expansão é Rousseff, que é seu sobrenome. Outra análise que pode ser feita é calcular quantos dos documentos retornados pela busca original também são retornados pela busca expandida. Neste caso, mesmo não adicionado os termos da consulta original e deixando apenas os termos que mais estão correlacionados para fazer a busca, a quantidade de documentos que estão na busca original e que não estão na busca expandida é mínima, o que mostra que esses termos estão bastante relacionados aos termos da consulta original.</p>\n",
    "\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "> <p style=\"text-align: justify\">A busca expandida retorna resultados melhores para a busca, pois como ela usa os termos que estão mais relacionados com os termos originais, fica mais expressiva e retornará também resultados de outras possíveis palavras chave que o usuário poderia usar em futuras buscas. Entretanto, quanto mais termos a busca expandida tiver, maior vai ser a quantidade de documentos retornados por ela, e se, não houver um rankeamento adequado dos resultados, o usuário pode levar mais tempo procurando os documentos que realmente são relevantes para ele.</p>\n",
    "\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n",
    "> <p style=\"text-align: justify\">Expandindo a consulta o recall irá melhorar, pois ele é a razão entre o total de documentos relevantes na busca e e a quantidade total de documentos relevantes, ou seja, quanto maior for a quantidade de documentos retornados maior será a quantidade de documentos relevantes que estarão contidos nessa busca. Por outro lado, a precisão diminui já que ela é a razão entre a quantidade de documentos relevantes na busca e a quantidade de documentos totais presentes na busca. Formula do recall: $\\frac{tp}{tp+fn}$; formula do precision: $\\frac{tp}{tp+fp}$; onde, tp é o total de documentos relevantes na busca, fn é o total de documentos relevantes não presentes na busca e fp é o total de documentos não relevantes na busca.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
